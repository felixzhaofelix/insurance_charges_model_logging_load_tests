{
 "cells": [
  {
   "cell_type": "raw",
   "id": "23017f05",
   "metadata": {},
   "source": [
    "Title: Logging for ML Model Deployments\n",
    "Date: 2023-04-20 12:00\n",
    "Category: Blog\n",
    "Slug: logging-for-ml-models\n",
    "Authors: Brian Schmidt\n",
    "Summary: As software systems become more and more complex, the people that build and operate these systems are finding that they are very hard to debug and inspect. To be able to solve this issue, a software system needs to be observable. An observable system is a system that allows an outside observer to infer the internal state of the system based purely on the data that it generates. The quality of \"observability\" helps the operators of a system to understand the inner workings of the system and to solve issues that may come up, even when the issues may be unprecedented. Just like any other software component, machine learning models need to create a log of events that may be useful later on. For example, we may want to know how many predictions the model made, how many errors occurred, and any other interesting events that we may want to keep track of. In this blog post we'll create a decorator that creates a log for a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9e777",
   "metadata": {},
   "source": [
    "# Logging for ML Model Deployments\n",
    "\n",
    "In previous blog posts we [introduced the decorator pattern](https://www.tekhnoal.com/ml-model-decorators.html) for ML model deployments and then showed how to use the pattern to build extensions for an ML model deployment. For example, in [this blog post](https://www.tekhnoal.com/data-enrichment-for-ml-models.html) we did data enrichment using a PostgreSQL database. The extensions were added without having to modify the machine learning model code at all, we were able to do it by using the decorator pattern. In this blog post we’ll add logging to a model deployment without having to modify the model code, using a decorator. \n",
    "\n",
    "This blog post is written in a Jupyter notebook and we'll be switching between Python code and shell commands, the formatting will reflect this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0503f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As software systems become more and more complex, the people that build and operate these systems are finding that they are very hard to debug and inspect. To be able to solve this issue, a software system needs to be observable. An observable system is a system that allows an outside observer to infer the internal state of the system based purely on the data that it generates. The quality of \"observability\" helps the operators of a system to understand the inner workings of the system and to solve issues that may come up, even when the issues may be unprecedented.\n",
    "\n",
    "Observability is a non-functional requirement (NFR) of a system. An NFR is a requirement that is placed on the operation of a system that has nothing to do with the specific functions of the system. Rather, it is a cross-cutting concern that needs to be addressed within the whole system design. Logging is a way that we can implement observability in a software system. \n",
    "\n",
    "In the world of software systems, a \"log\" is a record of events that happen as software runs. A log is made up of individual records called log records that each represent a single event in the software system. Logs are useful for debugging the system, keeping a permanent record of its activities, and many other purposes. In general, log records are designed for debugging, alerting, and auditing the activities of the system.\n",
    "\n",
    "Just like any other software component, machine learning models need to create a log of events that may be useful later on. For example, we may want to know how many predictions the model made, how many errors occurred, and any other interesting events that we may want to keep track of. In this blog post we'll create a decorator that creates a log for a machine learning model.\n",
    "\n",
    "This post is not meant to be a full guide for doing logging in Python, but we'll include some background information to make it easier to understand. Logging in Python can get complicated and there are other places that cover it more thoroughly. [Here](https://realpython.com/python-logging/) is a good place to learn more about Python logging.\n",
    "\n",
    "All of the code is available in [this github repository](https://github.com/schmidtbri/logging-for-ml-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587187c1",
   "metadata": {},
   "source": [
    "## Software Architecture\n",
    "\n",
    "The logging decorator will operate within the model service, but it requires outside services to handle the logs that it produces. This makes the software architecture more complicated and requires that we add several more services to the mix. \n",
    "\n",
    "![Software Architecture](software_architecture_lfmlm.png)\n",
    "![Software Architecture]({attach}software_architecture_lfmlm.png){ width=100% }\n",
    "\n",
    "The logging decorator is executing right after the prediction request is received from the client and a prediction is made by the model, it will send logs to be handled by other services. The other services are:\n",
    "\n",
    "- Log Forwarder: a service that runs on each cluster node that forwards logs from the local hard drive to the log aggregator service.\n",
    "- Log Storage: a service that can store logs and also query them.\n",
    "- Log User Interface: a service with a web interface that provides access to the logs stored in the log storage service.\n",
    "\n",
    "The specific services that we'll use will be detailed later in the blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fbac3",
   "metadata": {},
   "source": [
    "## Logging Best Practices\n",
    "\n",
    "There are certain things that we can do when we create a log for our application that makes it more useful, especially in production settings. For example, attaching a \"level\" to each log record makes it easy to filter the log according to the severity of the events. For example, a log record is of level \"INFO\" when it communicates a simple action that the system has taken. A \"WARNING\" log event is an event that may indicate a problem in the system, but the system can continue to run. A good description of the common log levels is [here](https://sematext.com/blog/logging-levels/).\n",
    "\n",
    "Another good practice for logs is to include contextual information that can help to debug any problems that may arise in the execution of the code. For example, we can include the location in the codebase where the log record was generated. This information is very helpful during debugging and helps to quickly find the code that caused the event to happen. The information is often presented as the function name, code file name, and line number where the log record was generated. Another piece of useful contextual information is the hostname of the machine where the log was generated.\n",
    "\n",
    "Logs should be easy to interpret for both humans and machines, this means that log records  are often written in text strings. Humans can easily read text, but parsing a text string is complicated for machines. To allow both humans and machines to easily parse a log message, a good middle ground is to use JSON formatting. JSON-formatted logs are easy to parse, but also allow a human to quickly read and understand a log message.\n",
    "\n",
    "Unique identifiers are useful to include in logs because they allow us to correlate many different log records together into a cohesive picture. For example, a correlation id is a unique ID that is generated to identify a specific transaction or query in a system. Adding unique identifiers to each log record can make it possible to debug complex problems that happen across system boundaries. A good description of correlation ids is [here](https://hilton.org.uk/blog/microservices-correlation-id)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c74e0",
   "metadata": {},
   "source": [
    "## Logging in Python\n",
    "\n",
    "The python standard library has a module that can simplify logging. The logging module is imported and used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.warning(\"Warning message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3efbe",
   "metadata": {},
   "source": [
    "To start logging, we instantiated a logger object using the logging.getLogger() function. Then we used the logger object to log a WARNING message.\n",
    "\n",
    "The log records are being sent to the stderr output of the process by default. We'll change that by instantiating a StreamHandler and pointing it at the stdout stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logger.addHandler(stream_handler)\n",
    "logger.warning(\"Warning message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4192a9",
   "metadata": {},
   "source": [
    "We just replaced the original log handler that logged messages to stderror with another one that logs to stdout. A log handler is a software component that is able to send log messages to destinations outside of the running process.\n",
    "\n",
    "We can also log messages at other levels, here is a WARNING and DEBUG message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(\"Warning message.\")\n",
    "logger.debug(\"Debug message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e2b34",
   "metadata": {},
   "source": [
    "When the code above executed, only the WARNING message was printed because the logger only sends log messages to the output that are at the WARNING level or above by default. This filtering functionality is helpful when you are only interested in logs above a certain level. We can change that by configuring the logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b301d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.warning(\"Warning message.\")\n",
    "logger.debug(\"Debug message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698dcbd",
   "metadata": {},
   "source": [
    "Now we can see the debug message. \n",
    "\n",
    "We can put in more information to the log record by adding a formatter to the log handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = logging.Formatter('%(asctime)s:%(name)s:%(levelname)s: %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "logger.warning(\"Warning message.\")\n",
    "logger.debug(\"Debug message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b381a",
   "metadata": {},
   "source": [
    "A formatter is a software component that can format log messages according to a desired format. The log record now contains the date and time of the event, the name of the logger that generated the message, the level of the log, and the log message. These are all standard fields that are attached to log messages when they are created, more information about these fields can be found in the Python documentation [here](https://docs.python.org/3/library/logging.html#logrecord-attributes).\n",
    "\n",
    "Each logger has a name attached to it when it is created, the name of the current logger is \"root\" because we created the logger without specifying a name. We can create a new logger with a name like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55baffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"test_logger\")\n",
    "\n",
    "logger.debug(\"Debug message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c8b04",
   "metadata": {},
   "source": [
    "The log record has the name of the logger, which is not the root logger that we were using before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa766a6c",
   "metadata": {},
   "source": [
    "### Logging Environment Variables\n",
    "\n",
    "To log extra information that is not available by default within each log record we have to extend the logging module by creating Filter classes. A Filter is simply a class that filters log records and can also modify them. This information will come from the environment variables of the process in which the logger is running. \n",
    "\n",
    "To do this we'll create a Filter that is able to pick up information from the environment variables and add it to each log record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from logging import Filter\n",
    "\n",
    "\n",
    "class EnvironmentInfoFilter(Filter):\n",
    "    \"\"\"Logging filter that adds information to log records from environment variables.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_variables: List[str]):\n",
    "        super().__init__()\n",
    "        self._env_variables = env_variables\n",
    "\n",
    "    def filter(self, record):\n",
    "        for env_variable in self._env_variables:\n",
    "            record.__setattr__(env_variable.lower(), os.environ.get(env_variable, \"N/A\"))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13352e",
   "metadata": {},
   "source": [
    "To try it out we'll have to add an environment variable that will be logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60011af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NODE_IP\"] = \"198.197.196.195\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ac84b",
   "metadata": {},
   "source": [
    "Next, we'll instantiate the Filter class and add it to a logger instance to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced21823",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_info_filter = EnvironmentInfoFilter(env_variables=[\"NODE_IP\"])\n",
    "\n",
    "logger.addFilter(environment_info_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = logging.Formatter('%(asctime)s : %(name)s : %(levelname)s : %(node_ip)s : %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "logger.warning(\"Warning message.\")\n",
    "logger.debug(\"Debug message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644cd2e",
   "metadata": {},
   "source": [
    "The log record now contains the IP address that we set in the environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae437b1",
   "metadata": {},
   "source": [
    "### Logging in JSON\n",
    "\n",
    "So far, the logs we've been generated have been in a slightly structured format that we came up with. It uses colons to separate out different sections of the log record. If we want to easily parse the logs to extract information from them, we should instead use JSON records. In this section we'll use the python-json-logger package to format the log records as JSON strings. \n",
    "\n",
    "First, we'll install the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install python-json-logger\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ab922",
   "metadata": {},
   "source": [
    "We'll instantiate a JsonFormatter object that will convert the logs to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "\n",
    "json_formatter = jsonlogger.JsonFormatter(\"%(asctime)s %(name)s %(levelname)s %(node_ip)s %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983cae09",
   "metadata": {},
   "source": [
    "We'll add the formatter to the stream handler that we created above like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handler.setFormatter(json_formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f822b",
   "metadata": {},
   "source": [
    "Now when we log, the output will be a JSON string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.error(\"Error message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7f3c9",
   "metadata": {},
   "source": [
    "We can add easily add more fields from the log record to make it more comprehensive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_formatter = jsonlogger.JsonFormatter(\"%(asctime)s %(node_ip)s %(process)s %(thread)s %(pathname)s %(lineno)s %(levelname)s %(message)s\")\n",
    "\n",
    "stream_handler.setFormatter(json_formatter)\n",
    "\n",
    "logger.error(\"Error message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a35a1",
   "metadata": {},
   "source": [
    "Some of these fields were added by the Filter that we built above, other fields are [default fields](https://docs.python.org/3/library/logging.html#logrecord-attributes) provided by the Python logging module.\n",
    "\n",
    "The JSON formatter can also add extra fields and values to the log record by using the \"extra\" parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = {\n",
    "    \"action\": \"predict\",\n",
    "    \"model_qualified_name\": \"model_qualified_name\",\n",
    "    \"model_version\": \"model_version\",\n",
    "    \"status\":\"error\",\n",
    "    \"error_info\": \"error_info\"\n",
    "}\n",
    "\n",
    "logger.error(\"message\", extra=extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722888a",
   "metadata": {},
   "source": [
    "The extra fields are:\n",
    "\n",
    "- action: the method called on the MLModel instance\n",
    "- model_qualified_name: the qualified name of the model\n",
    "- model_version: the version of the model\n",
    "- status: whether the action succeeded or not, can be \"success\" or \"error\"\n",
    "- error_info: extra error information, only present if an error occurred\n",
    "\n",
    "This information would normally be included in the \"message\" field of the log record as unstructured text, but by breaking it out and putting it into individual fields in the JSON log record we'll be able to parse it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740a147",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "We've done a few things with the logger module, now we need to put it all together into one configuration that we can use to set up the logger the way we want it.\n",
    "\n",
    "The logging.config.dictConfig() function can accept all of the options of the loggers, formatters, handlers, and filters and set them up with one function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d801277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "\n",
    "\n",
    "logging_config = {\n",
    "    \"version\": 1,\n",
    "    \"disable_existing_loggers\": True,\n",
    "    \"loggers\": {\n",
    "        \"root\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"handlers\": [\"stdout\"],\n",
    "            \"propagate\": False\n",
    "        }\n",
    "    },\n",
    "    \"filters\": {\n",
    "        \"environment_info_filter\": {\n",
    "            \"()\": \"__main__.EnvironmentInfoFilter\",\n",
    "            \"env_variables\": [\"NODE_IP\"]\n",
    "        }\n",
    "    },\n",
    "    \"formatters\": {\n",
    "        \"json_formatter\": {\n",
    "            \"class\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n",
    "            \"format\": \"%(asctime)s %(node_ip)s %(name)s %(pathname)s %(lineno)s %(levelname)s %(message)s\"\n",
    "        }\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"stdout\":{\n",
    "            \"level\":\"INFO\",\n",
    "            \"class\":\"logging.StreamHandler\",\n",
    "            \"stream\": \"ext://sys.stdout\",\n",
    "            \"formatter\": \"json_formatter\",\n",
    "            \"filters\": [\"environment_info_filter\"]\n",
    "        }\n",
    "    }    \n",
    "}\n",
    "\n",
    "logging.config.dictConfig(logging_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.debug(\"Debug message.\")\n",
    "logger.info(\"Info message.\")\n",
    "logger.error(\"Error message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377be12",
   "metadata": {},
   "source": [
    "The logger behaved in the same way as when we created it programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ccd2e",
   "metadata": {},
   "source": [
    "## Installing a Model\n",
    "\n",
    "We won't be training an ML model from scratch in this blog post because it would take a lot of space in the post. We'll be reusing a model that we built in a [previous blog post](https://www.tekhnoal.com/health-checks-for-ml-model-deployments.html). The model's code is hosted in [this github repository](https://github.com/schmidtbri/health-checks-for-ml-model-deployments). The model is used to predict credit risk.\n",
    "\n",
    "The model itself can be installed as a normal Python package, using the pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce98912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e git+https://github.com/felixzhaofelix/regression-model-fixed#egg=insurance_charges_model\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfaeda",
   "metadata": {},
   "source": [
    "Making a prediction with the model is done through the CreditRiskModel class, which we'll import like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.model import InsuranceChargesModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059299e",
   "metadata": {},
   "source": [
    "Now we'll instantiate the model class in order to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86688515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d11f3",
   "metadata": {},
   "source": [
    "In order to make a prediction with the model instance, we'll need to instantiate the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.schemas import InsuranceChargesModelInput\n",
    "\n",
    "model_input = InsuranceChargesModelInput(\n",
    "    age = 65,\n",
    "    sex = \"male\",\n",
    "    bmi = 50,\n",
    "    children = 5,\n",
    "    smoker = True,\n",
    "    region = \"northeast\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29a143",
   "metadata": {},
   "source": [
    "The model's input schema is called CreditRiskModelInput and it holds all of the features required by the model to make a prediction.\n",
    "\n",
    "Now we can make a prediction with the model by calling the predict() method with an instance of the CreditRiskModelInput class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fd91",
   "metadata": {},
   "source": [
    "The model predicts that the client's risk is safe.\n",
    "\n",
    "The output is also provided as an object, and because the model is a classification model, the output is an Enum. We can view the schema of the model output by requesting the JSON schema from the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4df0a4",
   "metadata": {},
   "source": [
    "The two possible outputs of the model are \"safe\" and \"risky\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628402f",
   "metadata": {},
   "source": [
    "## Creating the Logging Decorator\n",
    "\n",
    "As you saw above, the model did not produce any logs. To be able to emit some logs about the model's activity, we'll create a Decorator that will do logging around an MLModel instance. \n",
    "\n",
    "In order to build a MLModel decorator class, we'll need to inherit from the MLModelDecorator class and add some functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49868b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import logging\n",
    "from ml_base.decorator import MLModelDecorator\n",
    "from ml_base.ml_model import MLModelSchemaValidationException\n",
    "\n",
    "\n",
    "class LoggingDecorator(MLModelDecorator):\n",
    "    \"\"\"Decorator to do logging around an MLModel instance.\"\"\"\n",
    "\n",
    "    def __init__(self, input_fields: Optional[List[str]] = None, \n",
    "                 output_fields: Optional[List[str]] = None) -> None:\n",
    "        super().__init__(input_fields=input_fields, output_fields=output_fields)\n",
    "        self.__dict__[\"_logger\"] = None\n",
    "        \n",
    "    def predict(self, data):\n",
    "        if self.__dict__[\"_logger\"] is None:\n",
    "            self.__dict__[\"_logger\"] = logging.getLogger(\"{}_{}\".format(\n",
    "                self._model.qualified_name, \"logger\"))\n",
    "        \n",
    "        # extra fields to be added to the log record\n",
    "        extra = {\n",
    "            \"action\": \"predict\",\n",
    "            \"model_qualified_name\": self._model.qualified_name,\n",
    "            \"model_version\": self._model.version\n",
    "        }\n",
    "        \n",
    "        # adding model input fields to the extra fields to be logged\n",
    "        new_extra = dict(extra)\n",
    "        if self._configuration[\"input_fields\"] is not None:\n",
    "            for input_field in self._configuration[\"input_fields\"]:\n",
    "                new_extra[input_field] = getattr(data, input_field)\n",
    "        \n",
    "        self.__dict__[\"_logger\"].info(\"Prediction requested.\", extra=new_extra)\n",
    "        \n",
    "        try:\n",
    "            prediction = self._model.predict(data=data)\n",
    "            extra[\"status\"] = \"success\"\n",
    "            \n",
    "            # adding model output fields to the extra fields to be logged\n",
    "            new_extra = dict(extra)\n",
    "            if self._configuration[\"output_fields\"] is not None:\n",
    "                for output_field in self._configuration[\"output_fields\"]:\n",
    "                    new_extra[output_field] = getattr(prediction, output_field)            \n",
    "            self.__dict__[\"_logger\"].info(\"Prediction created.\", extra=new_extra) \n",
    "            return prediction\n",
    "        except Exception as e:\n",
    "            extra[\"status\"] = \"error\"\n",
    "            extra[\"error_info\"] = str(e)\n",
    "            self.__dict__[\"_logger\"].error(\"Prediction exception.\", extra=extra)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b35205",
   "metadata": {},
   "source": [
    "The LoggingDecorator class has most of its logic in the predict() method. This method simply instantiates a logger object and logs a message before a prediction is made, after it is made, and in the case when an exception is raised. Notice that the exception information is logged, but the exception is re-raised immediately after. We don't want to keep the exception from being handled by whatever code is using the model, we just need to emit a log of the event.\n",
    "\n",
    "The decorator also adds a few fields to the log message:\n",
    "\n",
    "- action: the action that the model is performing, in this case \"prediction\"\n",
    "- model_qualified_name: the qualified name of the model performing the action\n",
    "- model_version: the version of the model performing the action\n",
    "- status: the result of the action, can be either \"success\" or \"error\"\n",
    "- error_info: an optional field that adds error information when an exception is raised\n",
    "\n",
    "These fields are added on top of all the regular fields that the logging package provides. The extra information should allow us to easily filter logs later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60f1e3",
   "metadata": {},
   "source": [
    "## Decorating the Model\n",
    "\n",
    "To test out the decorator we’ll first instantiate the model object that we want to use with the decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401461a9",
   "metadata": {},
   "source": [
    "Next, we’ll instantiate the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_decorator = LoggingDecorator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5950edc",
   "metadata": {},
   "source": [
    "We can add the model instance to the decorator after it’s been instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fdd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_model = logging_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc37bd0",
   "metadata": {},
   "source": [
    "We can see the decorator and the model objects by printing the reference to the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa907e",
   "metadata": {},
   "source": [
    "The decorator object is printing out its own type along with the type of the model that it is decorating.\n",
    "\n",
    "Now we can try out the logging decorator by making a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d14a7",
   "metadata": {},
   "source": [
    "Calling the predict() method on the decorated model now emits two log messages. The first message is a \"Prediction requested.\" message and happens before the model's predict method is called. The second is a \"Prediction created.\" message and happens after the prediction is returned by the model to the decorator. The decorator can also log exceptions made by the model.\n",
    "\n",
    "The logging decorator is also able to grab fields from the model's input and output and log those alongside the other fields. Here is how to configure the logging decorator to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_decorator = LoggingDecorator(input_fields=[\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\"],\n",
    "                                     output_fields=[\"charges\"])\n",
    "\n",
    "decorated_model = logging_decorator.set_model(model)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed584261",
   "metadata": {},
   "source": [
    "The \"Prediction requested.\" log message now has two extra fields, the \"collections_in_last_12_months\" field and the \"debt_to_income_ratio\" field which were directly copied from the model input. The \"Prediction created.\" log message also has the \"credit_risk\" field, which is the prediction returned by the model.\n",
    "\n",
    "We now have a working logging decorator that can help us to do logging if the model does not do logging for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e91c47",
   "metadata": {},
   "source": [
    "## Adding the Decorator to a Deployed Model\n",
    "\n",
    "Now that we have a decorator that works locally, we can deploy it with a model inside of a service. The [rest_model_service package](https://pypi.org/project/rest-model-service/) is able to host ML models and create a RESTful API for each individual model. We don't need to write any code to do this because the service can decorate the models that it hosts with decorators that we provide. You can learn more about the package in [this blog post](https://www.tekhnoal.com/rest-model-service.html). You can learn how the rest_model_service package can be configured to add decorators to a model in [this blog post](https://www.tekhnoal.com/ml-model-decorators.html).\n",
    "\n",
    "To install the service package, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rest_model_service>=0.3.0\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8c690",
   "metadata": {},
   "source": [
    "The configuration for our model and decorator looks like this:\n",
    "\n",
    "```yaml\n",
    "service_title: Insurance Charges Model Service\n",
    "models:\n",
    "  - class_path: insurance_charges_model.prediction.model.InsuranceChargesModel\n",
    "    create_endpoint: true\n",
    "    decorators:\n",
    "      - class_path: ml_model_logging.logging_decorator.LoggingDecorator\n",
    "        configuration:\n",
    "          input_fields: [\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\"]\n",
    "          output_fields: [\"charges\"]\n",
    "logging:\n",
    "    version: 1\n",
    "    disable_existing_loggers: false\n",
    "    formatters:\n",
    "      json_formatter:\n",
    "        class: pythonjsonlogger.jsonlogger.JsonFormatter\n",
    "        format: \"%(asctime)s %(node_ip)s %(name)s %(levelname)s %(message)s\"\n",
    "    filters:\n",
    "      environment_info_filter:\n",
    "        \"()\": ml_model_logging.filters.EnvironmentInfoFilter\n",
    "        env_variables:\n",
    "        - NODE_IP\n",
    "    handlers:\n",
    "      stdout:\n",
    "        level: INFO\n",
    "        class: logging.StreamHandler\n",
    "        stream: ext://sys.stdout\n",
    "        formatter: json_formatter\n",
    "        filters:\n",
    "        - environment_info_filter\n",
    "    loggers:\n",
    "      root:\n",
    "        level: INFO\n",
    "        handlers:\n",
    "        - stdout\n",
    "        propagate: true\n",
    "```\n",
    "\n",
    "The two main sections in the file are the \"models\" section and the \"logging\" section. The models section is simpler and lists the CreditRiskModel, along with the LoggingDecorator. The decorators configuration simply adds an instance of the LoggingDecorator to the CreditRiskModel when the service starts up.\n",
    "\n",
    "The logging configuration is set up exactly like we set it up in the examples above except that it is in YAML format. The YAML is converted to a dictionary and passed directly into the logging.config.dictConfig() function.\n",
    "\n",
    "To run the service locally, execute these commands:\n",
    "\n",
    "```bash\n",
    "export NODE_IP=123.123.123.123\n",
    "export PYTHONPATH=./\n",
    "export REST_CONFIG=./configuration/rest_configuration.yaml\n",
    "uvicorn rest_model_service.main:app --reload\n",
    "```\n",
    "\n",
    "The NODE_IP environment variable is set so that the value can be added to the log messages through the filter we built above. The service should come up and can be accessed in a web browser at http://127.0.0.1:8000. When you access that URL you will be redirected to the documentation page that is generated by the FastAPI package:\n",
    "\n",
    "![Service Documentation](service_documentation_lfmlm.png)\n",
    "![Service Documentation]({attach}service_documentation_lfmlm.png){ width=100% }\n",
    "\n",
    "The documentation allows you to make requests against the API in order to try it out. Here's a prediction request against the insurance charges model:\n",
    "\n",
    "![Prediction Request](prediction_request_lfmlm.png)\n",
    "![Prediction Request]({attach}prediction_request_lfmlm.png){ width=100% }\n",
    "\n",
    "And the prediction result:\n",
    "\n",
    "![Prediction Response](prediction_response_lfmlm.png)\n",
    "![Prediction Response]({attach}prediction_response_lfmlm.png){ width=100% }\n",
    "\n",
    "\n",
    "The prediction made by the model had to go through the logging decorator that we configured into the service, so we got these two log records from the process:\n",
    "\n",
    "![Prediction Log](prediction_log_lfmlm.png)\n",
    "![Prediction Log]({attach}prediction_log_lfmlm.png){ width=100% }\n",
    "\n",
    "The local web service process emits the logs to stdout just as we configured it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ad9b7",
   "metadata": {},
   "source": [
    "## Deploying the Model Service\n",
    "\n",
    "Now that we have a working service that is running locally, we can work on deploying it to Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a Docker Image\n",
    "\n",
    "Kubernetes needs to have a Docker image in order to deploy something, we'll build an image using this Dockerfile:\n",
    "\n",
    "Félix Zhao: The Docker file used by the author can not be used directly as it does not contain the right sub-dependencies. I have to modify it to make it work. The modified Dockerfile is as follows:\n",
    "(Namely changed the FROM python:3.9-slim as base to FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9 as base)\n",
    "The new line uses the Docker image provided by the FastAPI package, which is a good starting point for a FastAPI application.\n",
    "\n",
    "```dockerfile\n",
    "# syntax=docker/dockerfile:1\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9 as base\n",
    "\n",
    "WORKDIR /dependencies\n",
    "\n",
    "# installing git because we need to install the model package from the github repository\n",
    "RUN apt-get update -y && \\\n",
    "    apt-get install -y --no-install-recommends git\n",
    "\n",
    "# creating and activating a virtual environment\n",
    "ENV VIRTUAL_ENV=/opt/venv\n",
    "RUN python3 -m venv $VIRTUAL_ENV\n",
    "ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n",
    "\n",
    "# installing dependencies\n",
    "COPY ./service_requirements.txt ./service_requirements.txt\n",
    "RUN pip install --no-cache -r service_requirements.txt\n",
    "\n",
    "FROM base as runtime\n",
    "\n",
    "ARG DATE_CREATED\n",
    "ARG REVISION\n",
    "ARG VERSION\n",
    "\n",
    "LABEL org.opencontainers.image.title=\"Logging for ML Models\"\n",
    "LABEL org.opencontainers.image.description=\"Logging for machine learning models.\"\n",
    "LABEL org.opencontainers.image.created=$DATE_CREATED\n",
    "LABEL org.opencontainers.image.authors=\"6666331+schmidtbri@users.noreply.github.com\"\n",
    "LABEL org.opencontainers.image.source=\"https://github.com/schmidtbri/logging-for-ml-models\"\n",
    "LABEL org.opencontainers.image.version=$VERSION\n",
    "LABEL org.opencontainers.image.revision=$REVISION\n",
    "LABEL org.opencontainers.image.licenses=\"MIT License\"\n",
    "LABEL org.opencontainers.image.base.name=\"python:3.9-slim\"\n",
    "\n",
    "WORKDIR /service\n",
    "\n",
    "# install packages\n",
    "RUN apt-get update -y && \\\n",
    "    apt-get install -y --no-install-recommends libgomp1 && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY --from=base /opt/venv ./venv\n",
    "\n",
    "COPY ./ml_model_logging ./ml_model_logging\n",
    "COPY ./LICENSE ./LICENSE\n",
    "\n",
    "ENV PATH /service/venv/bin:$PATH\n",
    "ENV PYTHONPATH=\"${PYTHONPATH}:/service\"\n",
    "\n",
    "CMD [\"uvicorn\", \"rest_model_service.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "The Dockerfile includes a set of labels from the [Open Containers annotations specification](https://github.com/opencontainers/image-spec/blob/main/annotations.md). Most of the labels are hardcoded in the Dockerfile, but there are three that we need to add from the outside: the date created, the version, and the revision. To do this we'll pull some information into environment variables:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d79c316158469d17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde329ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_CREATED=!date +\"%Y-%m-%d %T\"\n",
    "REVISION=!git rev-parse HEAD\n",
    "\n",
    "!echo \"$DATE_CREATED\"\n",
    "!echo \"$REVISION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef141e0",
   "metadata": {},
   "source": [
    "Now we can use the values to build the image. We'll also provide the version as a build argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build \\\n",
    "  --build-arg DATE_CREATED=\"$DATE_CREATED\" \\\n",
    "  --build-arg VERSION=\"0.1.0\" \\\n",
    "  --build-arg REVISION=\"$REVISION\" \\\n",
    "  -t insurance_charges_model_service:0.1.0 ..\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc78b4c",
   "metadata": {},
   "source": [
    "To find the image we just built, we'll search through the local docker images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b24a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images | grep insurance_charges_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3c4c1",
   "metadata": {},
   "source": [
    "Next, we'll start the image to see if everything is working as expected.\n",
    "Félix Zhao: Now there's a warning saying that the platform used is different but that's ok, amd64 works fine on the new Apple Silicone chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "    -p 8000:8000 \\\n",
    "    -e REST_CONFIG=./configuration/rest_configuration.yaml \\\n",
    "    -e NODE_IP=\"123.123.123.123\" \\\n",
    "    -v $(pwd)/../configuration:/service/configuration \\\n",
    "    --name insurance_charges_model_service \\\n",
    "    insurance_charges_model_service:0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8aa9e6",
   "metadata": {},
   "source": [
    "Notice that we added an environment variable called NODE_IP, this is just so we have a value to pull into the logs later, its not the real node IP address.\n",
    "\n",
    "The service is up and running in the docker container. To view the logs coming out of the process, we'll use the docker logs command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs insurance_charges_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208fc7e",
   "metadata": {},
   "source": [
    "As we expected, the logs are coming out in JSON format, although there are some that are not. These logs are being emitted from logger objects that were initialized before the rest_model_service package got a chance to be initialized.\n",
    "\n",
    "The service should be accessible on port 8000 of localhost, so we'll try to make a prediction using the curl command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/api/models/insurance_charges_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{ \\\n",
    "      \"age\": 65, \\\n",
    "      \"sex\": \"male\", \\\n",
    "      \"bmi\": 50.0, \\\n",
    "      \"children\": 5, \\\n",
    "      \"smoker\": true, \\\n",
    "      \"region\": \"southwest\" \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879e0cd",
   "metadata": {},
   "source": [
    "We're done with the docker container so we'll stop it and stop it and remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df19cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill insurance_charges_model_service\n",
    "!docker rm insurance_charges_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf23128",
   "metadata": {},
   "source": [
    "## Creating a Kubernetes Cluster\n",
    "\n",
    "To show the system in action, we’ll deploy the model service and the minio service to a Kubernetes cluster. A local cluster can be easily started by using [minikube](https://minikube.sigs.k8s.io/docs/). Installation instructions can be found [here](https://minikube.sigs.k8s.io/docs/start/).\n",
    "\n",
    "To start the minikube cluster execute this command:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Felix Zhao: With my configuration and platform(MacOS, ARM64, Homebrew), I used this command to install minikube"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f80d7ae568775"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !brew install minikube"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "666234554d216430"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To delete and restart minikube to update settings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e6e657dbf7c5e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!minikube delete"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbca17e6f07fdb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start minikube with desired allocated memory:\n",
    "Felix Zhao: The old credit risk model was only 9xxMB big, but the new one is 3GB big, so I have to allocate more memory to minikube."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0449ea795cd92d4"
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "70735cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T20:11:05.207380Z",
     "start_time": "2023-11-16T20:09:52.997147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄  minikube v1.32.0 on Darwin 13.0 (arm64)\r\n",
      "✨  Using the docker driver based on existing profile\r\n",
      "👍  Starting control plane node minikube in cluster minikube\r\n",
      "🚜  Pulling base image ...\r\n",
      "🤷  docker \"minikube\" container is missing, will recreate.\r\n",
      "🔥  Creating docker container (CPUs=2, Memory=6144MB) ...\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\r\n",
      "❗  This container is having trouble accessing https://registry.k8s.io\r\n",
      "💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/\r\n",
      "🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\u001B[K\r\n",
      "🔗  Configuring bridge CNI (Container Networking Interface) ...\r\n",
      "🔎  Verifying Kubernetes components...\r\n",
      "    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\r\n",
      "🌟  Enabled addons: storage-provisioner, default-storageclass\r\n",
      "🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\r\n"
     ]
    }
   ],
   "source": [
    "!minikube start --memory 6144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔌  Enabling dashboard ...\r\n",
      "    ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0\r\n",
      "    ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\r\n",
      "💡  Some dashboard features require the metrics-server addon. To enable all features please run:\r\n",
      "\r\n",
      "\tminikube addons enable metrics-server\t\r\n",
      "\r\n",
      "\r\n",
      "🤔  Verifying dashboard health ...\r\n",
      "🚀  Launching proxy ...\r\n",
      "🤔  Verifying proxy health ...\r\n",
      "🎉  Opening http://127.0.0.1:51233/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!minikube dashboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T20:41:21.620266Z",
     "start_time": "2023-11-16T20:23:51.195028Z"
    }
   },
   "id": "3e0900f16e1c472e"
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡  metrics-server is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.\r\n",
      "You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS\r\n",
      "    ▪ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.4\r\n",
      "🌟  The 'metrics-server' addon is enabled\r\n"
     ]
    }
   ],
   "source": [
    "!minikube addons enable metrics-server"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T21:02:11.505652Z",
     "start_time": "2023-11-16T21:02:09.741814Z"
    }
   },
   "id": "c348257388df94ad"
  },
  {
   "cell_type": "markdown",
   "id": "12798dc1",
   "metadata": {},
   "source": [
    "Let's view all of the pods running in the minikube cluster to make sure we can connect to it using the kubectl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5cb90c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T21:04:13.025483Z",
     "start_time": "2023-11-16T21:04:12.684472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE\r\n",
      "kube-system            coredns-5dd5756b68-rwgnc                     1/1     Running   2 (52m ago)   3h49m\r\n",
      "kube-system            etcd-minikube                                1/1     Running   1 (53m ago)   3h50m\r\n",
      "kube-system            kube-apiserver-minikube                      1/1     Running   1 (53m ago)   3h50m\r\n",
      "kube-system            kube-controller-manager-minikube             1/1     Running   1 (53m ago)   3h50m\r\n",
      "kube-system            kube-proxy-6xhn2                             1/1     Running   1 (53m ago)   3h49m\r\n",
      "kube-system            kube-scheduler-minikube                      1/1     Running   1 (53m ago)   3h50m\r\n",
      "kube-system            metrics-server-7c66d45ddc-pnxpj              1/1     Running   0             2m1s\r\n",
      "kube-system            storage-provisioner                          1/1     Running   3 (52m ago)   3h50m\r\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-7fd5cb4ddc-ttml5   1/1     Running   0             40m\r\n",
      "kubernetes-dashboard   kubernetes-dashboard-8694d4445c-n6nh5        1/1     Running   0             40m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d33987",
   "metadata": {},
   "source": [
    "Looks like we can connect, we're ready to start deploying the model service to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bf08c",
   "metadata": {},
   "source": [
    "### Creating a Namespace\n",
    "\n",
    "Now that we have a cluster and are connected to it, we'll create a namespace to hold the resources for our model deployment. The resource definition is in the kubernetes/namespace.yaml file. To apply the manifest to the cluster, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a64f4",
   "metadata": {},
   "source": [
    "To take a look at the namespaces, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d93a5337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T21:04:21.820327Z",
     "start_time": "2023-11-16T21:04:21.579704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS   AGE\r\n",
      "default                Active   3h50m\r\n",
      "kube-node-lease        Active   3h50m\r\n",
      "kube-public            Active   3h50m\r\n",
      "kube-system            Active   3h50m\r\n",
      "kubernetes-dashboard   Active   40m\r\n",
      "model-services         Active   3h50m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0a472",
   "metadata": {},
   "source": [
    "The new namespace should appear in the listing along with other namespaces created by default by the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e92982",
   "metadata": {},
   "source": [
    "### Creating the Model Service\n",
    "\n",
    "The model service is deployed by using Kubernetes resources. These are:\n",
    "\n",
    "- ConfigMap: a set of configuration options, in this case it is a simple YAML file that will be loaded into the running container as a volume mount. This resource allows us to change the configuration of the model service without having to modify the Docker image.\n",
    "- Deployment: a declarative way to manage a set of Pods, the model service pods are managed through the Deployment.\n",
    "- Service: a way to expose a set of Pods in a Deployment, the model service is made available to the outside world through the Service.\n",
    "\n",
    "These resources are defined in the kubernetes/model_service.yaml file, the file is long so we won't list it here. The env section in the container's definition in the Deployment has a special section which is allowing us to access information about the pod and the node:\n",
    "\n",
    "```yaml\n",
    "...\n",
    "- name: REST_CONFIG\n",
    "  value: ./configuration/kubernetes_rest_config.yaml\n",
    "- name: POD_NAME\n",
    "  valueFrom:\n",
    "    fieldRef:\n",
    "      fieldPath: metadata.name\n",
    "- name: NODE_NAME\n",
    "  valueFrom:\n",
    "    fieldRef:\n",
    "      fieldPath: spec.nodeName\n",
    "- name: APP_NAME\n",
    "  valueFrom:\n",
    "    fieldRef:\n",
    "      fieldPath: metadata.labels['app']\n",
    "...\n",
    "```\n",
    "\n",
    "The pod definition is using the [downward API provided by Kubernetes](https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/) to access the node name, the pod name, and the contents of the 'app' label. This information is made available as environment variables. We'll be adding this information to the log by adding the names of the environment variables to the logger configuration that we'll give to the model service. We built a logging context class above for the purpose of adding environment variables to log records.\n",
    "\n",
    "We're almost ready to deploy the model service, but before starting it we'll need to send the docker image from the local docker daemon to the minikube image cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3b4cd416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T21:12:08.333984Z",
     "start_time": "2023-11-16T21:07:22.576680Z"
    }
   },
   "outputs": [],
   "source": [
    "!minikube image load insurance_charges_model_service:0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e499d",
   "metadata": {},
   "source": [
    "We can view the images in the minikube cache with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "753dacca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T21:12:14.366162Z",
     "start_time": "2023-11-16T21:12:13.819426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker.io/library/insurance_charges_model_service:0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!minikube image ls | grep insurance_charges_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909afffb",
   "metadata": {},
   "source": [
    "The model service will need to access the YAML configuration file that we used for the local service above. This is file is in the /configuration folder and is called \"kubernetes_rest_config.yaml\", its customized for the kubernetes environment we're building.\n",
    "\n",
    "To create a [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) for the service, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9e86e74f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T21:19:44.913586Z",
     "start_time": "2023-11-16T21:19:44.578383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/model-service-configuration created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create configmap -n model-services model-service-configuration \\\n",
    "    --from-file=../configuration/kubernetes_rest_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          DATA   AGE\r\n",
      "model-service-configuration   1      61s\r\n",
      "Name:         model-service-configuration\r\n",
      "Namespace:    model-services\r\n",
      "Labels:       <none>\r\n",
      "Annotations:  <none>\r\n",
      "\r\n",
      "Data\r\n",
      "====\r\n",
      "kubernetes_rest_config.yaml:\r\n",
      "----\r\n",
      "service_title: Insurance Charges Model Service\r\n",
      "models:\r\n",
      "  - class_path: insurance_charges_model.prediction.model.InsuranceChargesModel\r\n",
      "    create_endpoint: true\r\n",
      "    decorators:\r\n",
      "      - class_path: ml_model_logging.logging_decorator.LoggingDecorator\r\n",
      "        configuration:\r\n",
      "          input_fields: [ \"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\" ]\r\n",
      "          output_fields: [ \"charges\" ]\r\n",
      "logging:\r\n",
      "    version: 1\r\n",
      "    disable_existing_loggers: false\r\n",
      "    formatters:\r\n",
      "      json_formatter:\r\n",
      "        class: pythonjsonlogger.jsonlogger.JsonFormatter\r\n",
      "        format: \"%(asctime)s %(pod_name)s %(node_name)s %(app_name)s %(name)s %(levelname)s %(message)s\"\r\n",
      "    filters:\r\n",
      "      environment_info_filter:\r\n",
      "        \"()\": ml_model_logging.filters.EnvironmentInfoFilter\r\n",
      "        env_variables:\r\n",
      "        - POD_NAME\r\n",
      "        - NODE_NAME\r\n",
      "        - APP_NAME\r\n",
      "    handlers:\r\n",
      "      stdout:\r\n",
      "        level: INFO\r\n",
      "        class: logging.StreamHandler\r\n",
      "        stream: ext://sys.stdout\r\n",
      "        formatter: json_formatter\r\n",
      "        filters:\r\n",
      "        - environment_info_filter\r\n",
      "    loggers:\r\n",
      "      root:\r\n",
      "        level: INFO\r\n",
      "        handlers:\r\n",
      "        - stdout\r\n",
      "        propagate: true\r\n",
      "\r\n",
      "\r\n",
      "BinaryData\r\n",
      "====\r\n",
      "\r\n",
      "Events:  <none>\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap -n model-services model-service-configuration\n",
    "!kubectl describe configmap -n model-services model-service-configuration"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T21:20:45.830516Z",
     "start_time": "2023-11-16T21:20:45.280740Z"
    }
   },
   "id": "1ab253167aa6ccf2"
  },
  {
   "cell_type": "markdown",
   "id": "146dda82",
   "metadata": {},
   "source": [
    "The service is deployed to the Kubernetes cluster with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9fd4dee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:00:39.423903Z",
     "start_time": "2023-11-16T23:00:38.837336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/insurance-charges-model-deployment configured\r\n",
      "service/insurance-charges-model-service unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -n model-services -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                      TAG       IMAGE ID       CREATED         SIZE\r\n",
      "insurance_charges_model_service                 0.1.0     d1acea47a0b5   5 days ago      3.13GB\r\n",
      "registry.k8s.io/kube-apiserver                  v1.28.3   537e9a59ee2f   4 weeks ago     120MB\r\n",
      "registry.k8s.io/kube-scheduler                  v1.28.3   42a4e73724da   4 weeks ago     57.8MB\r\n",
      "registry.k8s.io/kube-controller-manager         v1.28.3   8276439b4f23   4 weeks ago     116MB\r\n",
      "registry.k8s.io/kube-proxy                      v1.28.3   a5dd5cdd6d3e   4 weeks ago     68.3MB\r\n",
      "registry.k8s.io/metrics-server/metrics-server   <none>    24087ab2d904   3 months ago    66.9MB\r\n",
      "registry.k8s.io/etcd                            3.5.9-0   9cdd6470f48c   6 months ago    181MB\r\n",
      "registry.k8s.io/coredns/coredns                 v1.10.1   97e04611ad43   9 months ago    51.4MB\r\n",
      "registry.k8s.io/pause                           3.9       829e9de338bd   13 months ago   514kB\r\n",
      "kubernetesui/dashboard                          <none>    20b332c9a70d   14 months ago   244MB\r\n",
      "kubernetesui/metrics-scraper                    <none>    a422e0e98235   17 months ago   42.3MB\r\n",
      "gcr.io/k8s-minikube/storage-provisioner         v5        ba04bb24b957   2 years ago     29MB\r\n"
     ]
    }
   ],
   "source": [
    "!minikube ssh \"docker images\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:03:28.798799Z",
     "start_time": "2023-11-16T23:03:28.088670Z"
    }
   },
   "id": "950276191e862e3"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ index .Config.Labels \"org.opencontainers.image.source\" }\r\n"
     ]
    }
   ],
   "source": [
    "!docker inspect --format '{{ index .Config.Labels \"org.opencontainers.image.source\" }}' insurance_charges_model_service:0.1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:06:19.280985Z",
     "start_time": "2023-11-16T23:06:19.095441Z"
    }
   },
   "id": "25ec9d8ec2349214"
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "    {\r\n",
      "        \"Id\": \"sha256:d1acea47a0b5d400c27907744747a39c2fc45b70bc6233b4f0da11d1c4737176\",\r\n",
      "        \"RepoTags\": [\r\n",
      "            \"insurance_charges_model_service:0.1.0\"\r\n",
      "        ],\r\n",
      "        \"RepoDigests\": [],\r\n",
      "        \"Parent\": \"\",\r\n",
      "        \"Comment\": \"buildkit.dockerfile.v0\",\r\n",
      "        \"Created\": \"2023-11-11T20:42:06.436980422Z\",\r\n",
      "        \"Container\": \"\",\r\n",
      "        \"ContainerConfig\": {\r\n",
      "            \"Hostname\": \"\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": null,\r\n",
      "            \"Cmd\": null,\r\n",
      "            \"Image\": \"\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"\",\r\n",
      "            \"Entrypoint\": null,\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": null\r\n",
      "        },\r\n",
      "        \"DockerVersion\": \"\",\r\n",
      "        \"Author\": \"\",\r\n",
      "        \"Config\": {\r\n",
      "            \"Hostname\": \"\",\r\n",
      "            \"Domainname\": \"\",\r\n",
      "            \"User\": \"\",\r\n",
      "            \"AttachStdin\": false,\r\n",
      "            \"AttachStdout\": false,\r\n",
      "            \"AttachStderr\": false,\r\n",
      "            \"ExposedPorts\": {\r\n",
      "                \"80/tcp\": {}\r\n",
      "            },\r\n",
      "            \"Tty\": false,\r\n",
      "            \"OpenStdin\": false,\r\n",
      "            \"StdinOnce\": false,\r\n",
      "            \"Env\": [\r\n",
      "                \"PATH=/service/venv/bin:/opt/venv/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\r\n",
      "                \"LANG=C.UTF-8\",\r\n",
      "                \"GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568\",\r\n",
      "                \"PYTHON_VERSION=3.9.18\",\r\n",
      "                \"PYTHON_PIP_VERSION=23.0.1\",\r\n",
      "                \"PYTHON_SETUPTOOLS_VERSION=58.1.0\",\r\n",
      "                \"PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/c6add47b0abf67511cdfb4734771cbab403af062/public/get-pip.py\",\r\n",
      "                \"PYTHON_GET_PIP_SHA256=22b849a10f86f5ddf7ce148ca2a31214504ee6c83ef626840fde6e5dcd809d11\",\r\n",
      "                \"PYTHONPATH=/app:/service\",\r\n",
      "                \"VIRTUAL_ENV=/opt/venv\"\r\n",
      "            ],\r\n",
      "            \"Cmd\": [\r\n",
      "                \"uvicorn\",\r\n",
      "                \"rest_model_service.main:app\",\r\n",
      "                \"--host\",\r\n",
      "                \"0.0.0.0\",\r\n",
      "                \"--port\",\r\n",
      "                \"8000\"\r\n",
      "            ],\r\n",
      "            \"ArgsEscaped\": true,\r\n",
      "            \"Image\": \"\",\r\n",
      "            \"Volumes\": null,\r\n",
      "            \"WorkingDir\": \"/service\",\r\n",
      "            \"Entrypoint\": null,\r\n",
      "            \"OnBuild\": null,\r\n",
      "            \"Labels\": {\r\n",
      "                \"maintainer\": \"Sebastian Ramirez <tiangolo@gmail.com>\",\r\n",
      "                \"org.opencontainers.image.authors\": \"6666331+schmidtbri@users.noreply.github.com\",\r\n",
      "                \"org.opencontainers.image.base.name\": \"python:3.9-slim\",\r\n",
      "                \"org.opencontainers.image.created\": \"['2023-11-11 15:26:40']\",\r\n",
      "                \"org.opencontainers.image.description\": \"Logging for machine learning models.\",\r\n",
      "                \"org.opencontainers.image.licenses\": \"MIT License\",\r\n",
      "                \"org.opencontainers.image.revision\": \"['bfd0b72f8f81d86299c7a19f1666ad17b6b4fbe7']\",\r\n",
      "                \"org.opencontainers.image.source\": \"https://github.com/schmidtbri/logging-for-ml-models\",\r\n",
      "                \"org.opencontainers.image.title\": \"Logging for ML Models\",\r\n",
      "                \"org.opencontainers.image.version\": \"0.1.0\"\r\n",
      "            }\r\n",
      "        },\r\n",
      "        \"Architecture\": \"amd64\",\r\n",
      "        \"Os\": \"linux\",\r\n",
      "        \"Size\": 3129320857,\r\n",
      "        \"VirtualSize\": 3129320857,\r\n",
      "        \"GraphDriver\": {\r\n",
      "            \"Data\": {\r\n",
      "                \"LowerDir\": \"/var/lib/docker/overlay2/jdue3n56ula20o4vywzkwdcwi/diff:/var/lib/docker/overlay2/hpmo3sn9i1e093c3uo71qyxdd/diff:/var/lib/docker/overlay2/wpvcv6lg54prbqxsn8lqg11rf/diff:/var/lib/docker/overlay2/lm5icbmj40exafi310f04ptj5/diff:/var/lib/docker/overlay2/nrck44uygy4la596u8r9yjh0n/diff:/var/lib/docker/overlay2/ezv28orkadu6jk2m3a4m6oyc8/diff:/var/lib/docker/overlay2/olxejxn9x196t2en1a0ao9js8/diff:/var/lib/docker/overlay2/wi4zc5rnyq6xjc1w3wybpxhf0/diff:/var/lib/docker/overlay2/6yfp02a1iajmcv7dvcdsxw730/diff:/var/lib/docker/overlay2/8a051e87b8a676b9863fb33660c816a98ddad92a9a7d08b4e7ebbc07432eebb6/diff:/var/lib/docker/overlay2/0619f171e3fb1f32f69cc7da8cc7554993d9f6bde9204b6285142cceae647204/diff:/var/lib/docker/overlay2/e22dfce02bf31ba2ed352bcd605d08e218202bf3eda993efef5ffe407b01370e/diff:/var/lib/docker/overlay2/df6015271154c0bea02049aa82e64e5614903c598d7f66e344c8691a7563d5a1/diff:/var/lib/docker/overlay2/13be2585efa455649950e279de6264c6ddf1ec193b961603d2880b15b4c7f756/diff:/var/lib/docker/overlay2/13570bd272b63dd6bf79a07ed30b4ac7ec8ac5d81386c76ff6bc383dee69a4a9/diff:/var/lib/docker/overlay2/de8ca2e17945c5c9acc67f2b1606912cfbe2559628a637132eddb5875d9b83d5/diff:/var/lib/docker/overlay2/daee9ba6ecd8eb9af12688876b42864c8c7a163923f1bc18390c5d90d6e658ae/diff:/var/lib/docker/overlay2/df28c18b2c4b8bcb980c0d06821189516a628959e175de99c399ab906127a887/diff:/var/lib/docker/overlay2/7c79c4be632c494a850d5f83783ad26c6ee58f5859955afdc19333cdda66ccfa/diff:/var/lib/docker/overlay2/5acc1fcd3f21fca0e0a953a8c44271d94c1d8f6ad55d98748c27aa0fd3be3c9b/diff:/var/lib/docker/overlay2/4c4fe43425bcf4670674614710862cd654ceb81efd9014a2d70741149dcb7bf4/diff:/var/lib/docker/overlay2/4ae2819bde48047d77f696476a6dfe2e83ff35107e313b78ee780a09963532c0/diff:/var/lib/docker/overlay2/d424770733cd2390fd7224ee6b7780e00db61fecc1c2d4147c384bcd8c1cf186/diff:/var/lib/docker/overlay2/8ff3db7634d02970f0f2ce74b36e6fdb4b9137962d32a029b3560f018f99cfde/diff:/var/lib/docker/overlay2/6d3507a87e33d6e5f5c7c8c913460f1881f06918ba9957299fc8dcb5c284eccf/diff:/var/lib/docker/overlay2/e03f94780b1e59053d04ad992da4d419aab2badc0206597ae5586acfc1e3dde9/diff:/var/lib/docker/overlay2/19a005711ff2317358e13afa4f2cede731f549668c09477df566a66cd2397259/diff:/var/lib/docker/overlay2/a1c578e4b738e10668e52d2243986a6c9d338f4b4e0bf465b6b17c404aaa1650/diff:/var/lib/docker/overlay2/fdf24b1624465cb2c0b79a59ea3d60fff38b115c448852eb1436752754996a15/diff\",\r\n",
      "                \"MergedDir\": \"/var/lib/docker/overlay2/jqve8t4hbuxuw1kcv2393t4nt/merged\",\r\n",
      "                \"UpperDir\": \"/var/lib/docker/overlay2/jqve8t4hbuxuw1kcv2393t4nt/diff\",\r\n",
      "                \"WorkDir\": \"/var/lib/docker/overlay2/jqve8t4hbuxuw1kcv2393t4nt/work\"\r\n",
      "            },\r\n",
      "            \"Name\": \"overlay2\"\r\n",
      "        },\r\n",
      "        \"RootFS\": {\r\n",
      "            \"Type\": \"layers\",\r\n",
      "            \"Layers\": [\r\n",
      "                \"sha256:2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c\",\r\n",
      "                \"sha256:5cc3a4df1251c008ebfaaf8c48fe7bd61f0b0fdd7273966bd4bbe58877337e06\",\r\n",
      "                \"sha256:2ef3351afa6d94a8874a6af5aa1bafcd85106616ced8ac63112022fb04232d03\",\r\n",
      "                \"sha256:0c2d6fc19d6af33b608be1cccf7c50a78abcde56fdadc402edee9c2bf89b39bb\",\r\n",
      "                \"sha256:d3de4ba9f72c0f45112f9993bd617bba8bfb299420e526074a0748d35f44b0cb\",\r\n",
      "                \"sha256:6a4ba32696828d5359381fc70e1b46adf4c192a9b3528ff4621dd0ac9617e544\",\r\n",
      "                \"sha256:70981c1da3c1aa17bb20133520e44725b8341c4309203a42d0e70c6a4803d7f1\",\r\n",
      "                \"sha256:b343d97c2c3c5c1c663581dc5dae7f340f2d97bce8a36dc4fbe6cc77041edd23\",\r\n",
      "                \"sha256:ab7c86fb43cfd6131afda2b5ca1b0f5eb1e6eb0a05d1cedc29657203643314b4\",\r\n",
      "                \"sha256:d881f19d55a818e9257ec43b7c794722f264cdde8e52feb7183f6de2e9c5b67c\",\r\n",
      "                \"sha256:45cb49854146e9132c9d3f1e458b7d4171feb11cae1fb23cbbfb995a801a5918\",\r\n",
      "                \"sha256:3135758e06caeb89e15a71742ec8cd596d4318cfbf386b8949a5fabf5695b82a\",\r\n",
      "                \"sha256:01ae783ca49e101ebb86a1abfa992008cbad8eaa065e7500eae89317eccfcc45\",\r\n",
      "                \"sha256:5489950d99c3f3c8676346872b8d03e3c50718e22dbd53d37a66634da0fe0d0b\",\r\n",
      "                \"sha256:78cb771b07ce30b6c27c112b95298978febe4b40e373eb16cd631ee671dad1c6\",\r\n",
      "                \"sha256:740863a92291ab2d9dd70d0ba09f3c0ea9e149a269988daf8bfb95426f426545\",\r\n",
      "                \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\r\n",
      "                \"sha256:468ac5fc49ca9ad0111571660fa8b4d802e3ef6e806299b99a687b5cdb7235ab\",\r\n",
      "                \"sha256:58a38c80197136164cb21e68384acdea573db2320506b34db25bc77f520fc142\",\r\n",
      "                \"sha256:f453c67a3eafe728f0083304c8c1fc0064d8e7a5bd7c2047a4f6aaef7e4e7036\",\r\n",
      "                \"sha256:9860df33a984f04baa13f93d057b4ae11606b2608b4ac1641aeacd81c344088a\",\r\n",
      "                \"sha256:a4b9c453cca2a05d8deffc2b6bcb9fd63fa2dc74c809a77e2a0f0f672e867327\",\r\n",
      "                \"sha256:4086cfae94b8466068ddf7fe352f063816ff214d6bb671e07c459d2c402ba7a1\",\r\n",
      "                \"sha256:8bfcde5973663b32fb4febd88ca9bae114a0789b383297b49a86ffd3b688b806\",\r\n",
      "                \"sha256:c066ed897cebd85a6dd99118455a58ea2ebf1a5d55e573091e80d7c2ff6cf992\",\r\n",
      "                \"sha256:2128c036619d2642ae7e2696acccdf7919cf1c49a9f3330d5f3308277d5a7ff5\",\r\n",
      "                \"sha256:299f48a1f2b810e49ff7f1b15d296bd40c37ef6834e181a6cdc66cc70a0dfea0\",\r\n",
      "                \"sha256:2f98e7105affe4d1d959bb8cc3ffe448cce1a9b82d97d74efc10a3e1405ff1d6\",\r\n",
      "                \"sha256:9e573dafe5ac336a8f0714af6e89e2e33e607d210e873d6fdf761a69efb11ca6\",\r\n",
      "                \"sha256:2480d6d2f4491aa2f9b2e165a73a6a3a5d6fbb453f962380e987443dbd155112\"\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"Metadata\": {\r\n",
      "            \"LastTagTime\": \"2023-11-11T20:42:11.411355425Z\"\r\n",
      "        }\r\n",
      "    }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!docker image inspect insurance_charges_model_service:0.1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:07:06.220668Z",
     "start_time": "2023-11-16T23:07:06.035275Z"
    }
   },
   "id": "f2e8f8df76481381"
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"insurance-charges-model-deployment\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete deployment insurance-charges-model-deployment -n model-services"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T22:22:44.831631Z",
     "start_time": "2023-11-16T22:22:44.644669Z"
    }
   },
   "id": "2d24a22085398ffb"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: Metrics not available for pod model-services/insurance-charges-model-deployment-648489b69b-ftlgv, age: 37m46.427691s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl top pods -n model-services"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:00:43.542709Z",
     "start_time": "2023-11-16T23:00:43.334712Z"
    }
   },
   "id": "ec39ede7ed767e"
  },
  {
   "cell_type": "markdown",
   "id": "ad273498",
   "metadata": {},
   "source": [
    "The deployment and service for the model service were created together. Lets view the Deployment to see if it is available yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d680ae85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:00:47.081050Z",
     "start_time": "2023-11-16T23:00:46.896859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "insurance-charges-model-deployment   0/1     1            0           37m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments -n model-services "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be82f27",
   "metadata": {},
   "source": [
    "You can also view the pods that are running the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "aa3e77c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T22:24:16.397770Z",
     "start_time": "2023-11-16T22:24:16.211894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                  READY   STATUS             RESTARTS   AGE\r\n",
      "insurance-charges-model-deployment-648489b69b-ftlgv   0/1     ImagePullBackOff   0          79s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n model-services -l app=insurance-charges-model-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"insurance-charges-model-deployment-859b6fd6c-x5knw\" deleted\r\n",
      "pod \"insurance-charges-model-deployment-76c5584697-tx6zg\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod insurance-charges-model-deployment-648489b69b-ftlgv --grace-period=0 -n model-services"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T21:34:44.010946Z",
     "start_time": "2023-11-16T21:34:41.707427Z"
    }
   },
   "id": "63a1750214b9a619"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the service was not successfully deployed, let's first the check the pod info:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf20f3f4deb45f81"
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             insurance-charges-model-deployment-648489b69b-ftlgv\r\n",
      "Namespace:        model-services\r\n",
      "Priority:         0\r\n",
      "Service Account:  default\r\n",
      "Node:             minikube/192.168.49.2\r\n",
      "Start Time:       Thu, 16 Nov 2023 17:22:57 -0500\r\n",
      "Labels:           app=insurance-charges-model-service\r\n",
      "                  pod-template-hash=648489b69b\r\n",
      "Annotations:      <none>\r\n",
      "Status:           Pending\r\n",
      "IP:               10.244.0.14\r\n",
      "IPs:\r\n",
      "  IP:           10.244.0.14\r\n",
      "Controlled By:  ReplicaSet/insurance-charges-model-deployment-648489b69b\r\n",
      "Containers:\r\n",
      "  insurance-charges-model:\r\n",
      "    Container ID:   \r\n",
      "    Image:          insurance_charges_model_service:0.1.0\r\n",
      "    Image ID:       \r\n",
      "    Port:           80/TCP\r\n",
      "    Host Port:      0/TCP\r\n",
      "    State:          Waiting\r\n",
      "      Reason:       ImagePullBackOff\r\n",
      "    Ready:          False\r\n",
      "    Restart Count:  0\r\n",
      "    Limits:\r\n",
      "      cpu:     200m\r\n",
      "      memory:  2Gi\r\n",
      "    Requests:\r\n",
      "      cpu:      100m\r\n",
      "      memory:   500Mi\r\n",
      "    Liveness:   http-get http://:8000/api/health delay=0s timeout=2s period=10s #success=1 #failure=5\r\n",
      "    Readiness:  http-get http://:8000/api/health/ready delay=0s timeout=2s period=10s #success=1 #failure=5\r\n",
      "    Startup:    http-get http://:8000/api/health/startup delay=2s timeout=2s period=5s #success=1 #failure=5\r\n",
      "    Environment:\r\n",
      "      REST_CONFIG:  ./configuration/kubernetes_rest_config.yaml\r\n",
      "      POD_NAME:     insurance-charges-model-deployment-648489b69b-ftlgv (v1:metadata.name)\r\n",
      "      NODE_NAME:     (v1:spec.nodeName)\r\n",
      "      APP_NAME:      (v1:metadata.labels['app'])\r\n",
      "    Mounts:\r\n",
      "      /service/configuration from config-volume (rw)\r\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5vpj9 (ro)\r\n",
      "Conditions:\r\n",
      "  Type              Status\r\n",
      "  Initialized       True \r\n",
      "  Ready             False \r\n",
      "  ContainersReady   False \r\n",
      "  PodScheduled      True \r\n",
      "Volumes:\r\n",
      "  config-volume:\r\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n",
      "    Name:      model-service-configuration\r\n",
      "    Optional:  false\r\n",
      "  kube-api-access-5vpj9:\r\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n",
      "    TokenExpirationSeconds:  3607\r\n",
      "    ConfigMapName:           kube-root-ca.crt\r\n",
      "    ConfigMapOptional:       <nil>\r\n",
      "    DownwardAPI:             true\r\n",
      "QoS Class:                   Burstable\r\n",
      "Node-Selectors:              <none>\r\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\n",
      "Events:\r\n",
      "  Type     Reason     Age                 From               Message\r\n",
      "  ----     ------     ----                ----               -------\r\n",
      "  Normal   Scheduled  38m                 default-scheduler  Successfully assigned model-services/insurance-charges-model-deployment-648489b69b-ftlgv to minikube\r\n",
      "  Normal   Pulling    36m (x4 over 38m)   kubelet            Pulling image \"insurance_charges_model_service:0.1.0\"\r\n",
      "  Warning  Failed     36m (x4 over 38m)   kubelet            Failed to pull image \"insurance_charges_model_service:0.1.0\": Error response from daemon: pull access denied for insurance_charges_model_service, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\r\n",
      "  Warning  Failed     36m (x4 over 38m)   kubelet            Error: ErrImagePull\r\n",
      "  Warning  Failed     36m (x6 over 38m)   kubelet            Error: ImagePullBackOff\r\n",
      "  Normal   BackOff    30m (x20 over 38m)  kubelet            Back-off pulling image \"insurance_charges_model_service:0.1.0\"\r\n"
     ]
    }
   ],
   "source": [
    "# !kubectl get pods -n model-services\n",
    "!kubectl describe pod insurance-charges-model-deployment-648489b69b-ftlgv -n model-services"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:01:01.928837Z",
     "start_time": "2023-11-16T23:01:01.710907Z"
    }
   },
   "id": "e7da812528e05818"
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\r\n",
      "insurance_charges_model_service                           0.1.0                                                                        d1acea47a0b5   5 days ago      3.13GB\r\n",
      "gcr.io/k8s-minikube/kicbase                               v0.0.42                                                                      62753ecb37c4   10 days ago     1.11GB\r\n",
      "hubproxy.docker.internal:5555/docker/desktop-kubernetes   kubernetes-v1.28.2-cni-v1.3.0-critools-v1.28.0-cri-dockerd-v0.3.4-1-debian   19d261e62622   5 weeks ago     411MB\r\n",
      "getting-started                                           latest                                                                       dd4929ca15f1   6 weeks ago     265MB\r\n",
      "python                                                    latest                                                                       c7dde58e0c84   6 weeks ago     1.02GB\r\n",
      "registry.k8s.io/kube-apiserver                            v1.28.2                                                                      30bb499447fe   2 months ago    120MB\r\n",
      "registry.k8s.io/kube-proxy                                v1.28.2                                                                      7da62c127fc0   2 months ago    68.3MB\r\n",
      "registry.k8s.io/kube-controller-manager                   v1.28.2                                                                      89d57b83c178   2 months ago    116MB\r\n",
      "registry.k8s.io/kube-scheduler                            v1.28.2                                                                      64fc40cee371   2 months ago    57.8MB\r\n",
      "docker/desktop-vpnkit-controller                          dc331cb22850be0cdd97c84a9cfecaf44a1afb6e                                     3750dfec169f   6 months ago    35MB\r\n",
      "hubproxy.docker.internal:5555/docker/desktop-kubernetes   kubernetes-v1.25.9-cni-v1.1.1-critools-v1.25.0-cri-dockerd-v0.2.6-1-debian   72db983f29ff   7 months ago    385MB\r\n",
      "registry.k8s.io/kube-apiserver                            v1.25.9                                                                      b781b2ba4d81   7 months ago    123MB\r\n",
      "registry.k8s.io/kube-scheduler                            v1.25.9                                                                      591d2f8e126e   7 months ago    49.4MB\r\n",
      "registry.k8s.io/kube-proxy                                v1.25.9                                                                      7d06ab3388e9   7 months ago    58.1MB\r\n",
      "registry.k8s.io/kube-controller-manager                   v1.25.9                                                                      97b0bebd519d   7 months ago    113MB\r\n",
      "mcr.microsoft.com/azure-sql-edge                          latest                                                                       9d0e27694fc9   9 months ago    1.84GB\r\n",
      "registry.k8s.io/coredns/coredns                           v1.10.1                                                                      97e04611ad43   9 months ago    51.4MB\r\n",
      "registry.k8s.io/etcd                                      3.5.7-0                                                                      24bc64e91103   9 months ago    181MB\r\n",
      "docker/getting-started                                    latest                                                                       289dc403af49   10 months ago   46.5MB\r\n",
      "registry.k8s.io/etcd                                      3.5.6-0                                                                      ef2458028240   11 months ago   181MB\r\n",
      "registry.k8s.io/pause                                     3.9                                                                          829e9de338bd   13 months ago   514kB\r\n",
      "registry.k8s.io/pause                                     3.8                                                                          4e42fb3c9d90   17 months ago   514kB\r\n",
      "registry.k8s.io/coredns/coredns                           v1.9.3                                                                       b19406328e70   17 months ago   47.7MB\r\n",
      "docker/desktop-storage-provisioner                        v2.0                                                                         c027a58fa0bb   2 years ago     39.8MB\r\n"
     ]
    }
   ],
   "source": [
    "#!minikube ssh docker image inspect insurance_charges_model_service:0.1.0\n",
    "!docker images"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T23:01:25.410535Z",
     "start_time": "2023-11-16T23:01:25.211639Z"
    }
   },
   "id": "38c22d16ce5e2f6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kubectl describe deployment insurance-charges-model-deployment -n model-services"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6aa1d68a78f03ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kubectl logs -n model-services insurance-charges-model-deployment-544c5888dd-46xt2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbad83eac0c2a7d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kubectl logs -n model-services insurance-charges-model-deployment-544c5888dd-46xt2 --all-containers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1350479c4d147246"
  },
  {
   "cell_type": "markdown",
   "id": "bdc80259",
   "metadata": {},
   "source": [
    "The Kubernetes Service details look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services -n model-services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!minikube stop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc2930eb7f863b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3a768b470ed5dc"
  },
  {
   "cell_type": "markdown",
   "id": "8b246e55",
   "metadata": {},
   "source": [
    "We'll run a proxy process locally to be able to access the model service endpoint:\n",
    "\n",
    "```bash\n",
    "minikube service insurance-charges-model-service --url -n model-services\n",
    "```\n",
    "\n",
    "The command outputs this URL:\n",
    "\n",
    "http://127.0.0.1:50222\n",
    "\n",
    "We can send a request to the model service through the local endpoint like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff90ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X 'POST' \\\n",
    "    'http://127.0.0.1:8000/api/models/insurance_charges_model/prediction' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{ \\\n",
    "      \"age\": 65, \\\n",
    "      \"sex\": \"male\", \\\n",
    "      \"bmi\": 50.0, \\\n",
    "      \"children\": 5, \\\n",
    "      \"smoker\": true, \\\n",
    "      \"region\": \"southwest\" \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3d7c4",
   "metadata": {},
   "source": [
    "The model is deployed within Kubernetes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae461e9c",
   "metadata": {},
   "source": [
    "### Accessing the Logs\n",
    "\n",
    "Kubernetes has a built-in system that receives the stdout and stderr outputs of the running containers and saves them to the hard drive of the node for a limited time. You can view the logs emitted by the containers by using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be87a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl logs -n model-services insurance-charges-model-deployment-554575f4f-5rl5s -c insurance-charges-model | grep \"\\\"action\\\": \\\"predict\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0ccc1",
   "metadata": {},
   "source": [
    "The logs contain every field that we configured and they are in JSON format, as we expected. The log records also contain the pod_name, node_name, and app_name fields that we added through the downward API.\n",
    "\n",
    "Although we can view the logs like this, this is not the ideal way to hold logs. We need to be able to search through the logs generated across the whole system. To do this we'll need to export the logs to an external logging system. We'll be working on that in another section of this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5254fc",
   "metadata": {},
   "source": [
    "## Creating the Logging System\n",
    "\n",
    "The complexity of modern cloud environment makes it hard to manage logs in individual servers since we really don't know where our workloads are going to be scheduled ahead of time. Kubernetes workloads are highly distributed, meaning that an application can be replicated in many different nodes in a cluster. This makes it necessary to gather logs together in one place so that we can more easily view and analyze them.\n",
    "\n",
    "A logging system is responsible for gathering  log records from all of the instances of a running application and make them searchable from one centralized location. In this section, we'll add such a logging system to the cluster and use it to monitor the model service we've deployed.\n",
    "\n",
    "We'll be installing the Elastic Cloud on Kubernetes operator in order to view our logs. The operator installs and manages ElasticSearch, Kibana, and Filebeat services.\n",
    "\n",
    "To begin, lets install the [custom resource definitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) needed by the operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f https://download.elastic.co/downloads/eck/2.7.0/crds.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2bdfc",
   "metadata": {},
   "source": [
    "We'll be using theses CRDs:\n",
    "\n",
    "- elasticsearch.k8s.elastic.co, to deploy ElasticSearch for storing and indexing logs\n",
    "- kibana.k8s.elastic.co, to deploy Kibana for viewing logs\n",
    "- beat.k8s.elastic.co, to deploy Filebeat on each node to forward logs to ElasticSearch\n",
    "\n",
    "The CRDs are used by the ECK operator to manage resources in the cluster. To install the ECK operator itself, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f https://download.elastic.co/downloads/eck/2.7.0/operator.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf075d",
   "metadata": {},
   "source": [
    "### ElasticSearch \n",
    "\n",
    "We'll be storing logs in [ElasticSearch](https://www.elastic.co/elasticsearch/). ElasticSearch is a distributed full-text search engine with a RESTful API. The ElasticSearch service is ideal for our needs because our logs are made up of text strings.\n",
    "\n",
    "Now we're ready to install the service by applying the \"ElasticSearch\" custom resource definition:\n",
    "\n",
    "```yaml\n",
    "apiVersion: elasticsearch.k8s.elastic.co/v1\n",
    "kind: Elasticsearch\n",
    "metadata:\n",
    "  name: quickstart\n",
    "spec:\n",
    "  version: 8.7.0\n",
    "  nodeSets:\n",
    "  - name: default\n",
    "    count: 1\n",
    "    config:\n",
    "      node.store.allow_mmap: false\n",
    "```\n",
    "The CRD is stored in the kubernetes/elastic_search.yaml file. The CRD is applied with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -n elastic-system -f ../kubernetes/elastic_search.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c970559",
   "metadata": {},
   "source": [
    "To get a list of ElasticSearch clusters currently defined in the cluster, execute this comand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get elasticsearch -n elastic-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f85767",
   "metadata": {},
   "source": [
    "We can look at the pods running the ElasticSearch cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n elastic-system --selector='elasticsearch.k8s.elastic.co/cluster-name=quickstart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d21e5",
   "metadata": {},
   "source": [
    "A Kubernetes service is created to make the ElasticSearch service available to other services in the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8673167",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service quickstart-es-http -n elastic-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1cc12",
   "metadata": {},
   "source": [
    "A user named \"elastic\" is automatically in the ElasticSearch services with the password stored in a Kubernetes secret. Let's access the password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ade75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get secret quickstart-es-elastic-user -n elastic-system -o=jsonpath='{.data.elastic}' | base64 --decode; echo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b0ee3",
   "metadata": {},
   "source": [
    "### Kibana\n",
    "\n",
    "To view the logs we'll be using [Kibana](https://www.elastic.co/kibana/). Kibana is a web application that can provide access to and visualize logs stored in ElasticSearch.\n",
    "\n",
    "The CRD for Kibana looks like this:\n",
    "\n",
    "```yaml\n",
    "apiVersion: kibana.k8s.elastic.co/v1\n",
    "kind: Kibana\n",
    "metadata:\n",
    "  name: quickstart\n",
    "spec:\n",
    "  version: 8.7.0\n",
    "  count: 1\n",
    "  elasticsearchRef:\n",
    "    name: quickstart\n",
    "```\n",
    "\n",
    "We'll apply the CRD with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -n elastic-system -f ../kubernetes/kibana.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827a8d9",
   "metadata": {},
   "source": [
    "Similar to Elasticsearch, you can retrieve details about Kibana instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get kibana -n elastic-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3d7a6",
   "metadata": {},
   "source": [
    "We can also view the associated Pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod -n elastic-system --selector='kibana.k8s.elastic.co/name=quickstart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e99c4",
   "metadata": {},
   "source": [
    "A ClusterIP Service is automatically created for Kibana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service quickstart-kb-http -n elastic-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6efbd2",
   "metadata": {},
   "source": [
    "We'll use kubectl port-forward to access Kibana from a local web browser:\n",
    "\n",
    "```bash\n",
    "kubectl port-forward service/quickstart-kb-http 5601 -n elastic-system\n",
    "```\n",
    "\n",
    "Now we can access the Kibana service from this URL:\n",
    "\n",
    "```\n",
    "http://localhost:5601\n",
    "```\n",
    "\n",
    "Open the URL in your browser to view the Kibana UI. Login as the \"elastic\" user. The password is the one we retrieved above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b57be1",
   "metadata": {},
   "source": [
    "### Filebeat\n",
    "\n",
    "In order to centralize access to logs, we'll first need a way to get the logs off of the individual cluster nodes and forward them to the aggregator service. The service we'll use to do this is called [Filebeat](https://www.elastic.co/beats/filebeat). Filebeat is a lightweight service that can forward logs stored in files to an outside service. We'll deploy Filebeat as a DaemonSet to ensure there’s a running instance on each node of the cluster.\n",
    "\n",
    "The Filebeat CRD looks like this:\n",
    "\n",
    "\n",
    "```yaml\n",
    "apiVersion: beat.k8s.elastic.co/v1beta1\n",
    "kind: Beat\n",
    "metadata:\n",
    "  name: quickstart\n",
    "spec:\n",
    "  type: filebeat\n",
    "  version: 8.7.0\n",
    "  elasticsearchRef:\n",
    "    name: quickstart\n",
    "  kibanaRef:\n",
    "    name: quickstart\n",
    "  config:\n",
    "    processors:\n",
    "      - decode_json_fields:\n",
    "          fields: [\"message\"]\n",
    "          max_depth: 3\n",
    "          target: parsed_message\n",
    "          add_error_key: false\n",
    "    filebeat.inputs:\n",
    "    - type: container\n",
    "      paths:\n",
    "      - /var/log/containers/*.log\n",
    "  daemonSet:\n",
    "    podTemplate:\n",
    "      spec:\n",
    "        dnsPolicy: ClusterFirstWithHostNet\n",
    "        hostNetwork: true\n",
    "        securityContext:\n",
    "          runAsUser: 0\n",
    "        containers:\n",
    "        - name: filebeat\n",
    "          volumeMounts:\n",
    "          - name: varlogcontainers\n",
    "            mountPath: /var/log/containers\n",
    "          - name: varlogpods\n",
    "            mountPath: /var/log/pods\n",
    "          - name: varlibdockercontainers\n",
    "            mountPath: /var/lib/docker/containers\n",
    "        volumes:\n",
    "        - name: varlogcontainers\n",
    "          hostPath:\n",
    "            path: /var/log/containers\n",
    "        - name: varlogpods\n",
    "          hostPath:\n",
    "            path: /var/log/pods\n",
    "        - name: varlibdockercontainers\n",
    "          hostPath:\n",
    "            path: /var/lib/docker/containers\n",
    "```\n",
    "\n",
    "The container logs host folder (/var/log/containers) is mounted on the Filebeat container. The filebeat process also has a processor defined:\n",
    "\n",
    "- decode_json_fields, which decodes fields containing JSON strings and replaces the strings with valid JSON objects\n",
    "\n",
    "Let's apply the CRD to create the Filebeat DaemonSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250014d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -n elastic-system -f ../kubernetes/filebeat.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd874a",
   "metadata": {},
   "source": [
    "Details about the Filebeat service can be viewed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a29db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get beat -n elastic-system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859d0b8",
   "metadata": {},
   "source": [
    "The pods running the service can be listed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n elastic-system --selector='beat.k8s.elastic.co/name=quickstart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77d2df",
   "metadata": {},
   "source": [
    "The Filebeat service is running on the single node in the cluster.\n",
    "\n",
    "The logs are being forwarded to ElasticSearch and can be viewed in Kibana:\n",
    "\n",
    "![Prediction Log Stream](log_stream_lfmlm.png)\n",
    "![Prediction Log Stream]({attach}log_stream_lfmlm.png){ width=100% }\n",
    "\n",
    "We have logs arriving from the model service and can view them in Kibana!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5bcfe",
   "metadata": {},
   "source": [
    "## Deleting the Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9b6d3",
   "metadata": {},
   "source": [
    "To delete the Filebeat DaemonSet, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91feb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -n elastic-system -f ../kubernetes/filebeat.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19af9ed",
   "metadata": {},
   "source": [
    "To delete the Kibana service, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87469759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -n elastic-system -f ../kubernetes/kibana.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988494b4",
   "metadata": {},
   "source": [
    "To delete the ElasticSearch service, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f00d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -n elastic-system -f ../kubernetes/elastic_search.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0e6b",
   "metadata": {},
   "source": [
    "To remove all Elastic resources in all namespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3158bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get namespaces --no-headers -o custom-columns=:metadata.name | xargs -n1 kubectl delete elastic --all -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8b9ae",
   "metadata": {},
   "source": [
    "To uninstall the ECK operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f https://download.elastic.co/downloads/eck/2.7.0/operator.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7bff7",
   "metadata": {},
   "source": [
    "To uninstall the Custom Resource Definitions for the ECK operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f https://download.elastic.co/downloads/eck/2.7.0/crds.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78903c33",
   "metadata": {},
   "source": [
    "To delete the model service kubernetes resources, we'll execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -n model-services -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5b517",
   "metadata": {},
   "source": [
    "We'll also delete the ConfigMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -n model-services configmap model-service-configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f64c43",
   "metadata": {},
   "source": [
    "Then the model service namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5186f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ae158",
   "metadata": {},
   "source": [
    "To shut down the minikube cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52f6a0",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "In this blog post we showed how to do logging with the Python logging package, and how to create a decorator that can help us to do logging around an MLModel instance. We also set up and used a logging system within a Kubernetes cluster and used it to aggregate logs and view them. Logging is usually the first thing that is implemented when we need to monitor how a system performs, and machine learning models are no exception to this. The logging decorator allowed us to do complex logging without having to modify the implementation of the model at all, thus simplifying a common aspect of software observability.\n",
    "\n",
    "One of the benefits of using the decorator pattern is that we are able to build up complex behaviors around an object. The LoggingDecorator class is very configurable, since we are able to configure it to log input and output fields from the model. This approach makes the implementation very flexible, since we do not need to modify the decorator's code to add fields to the log. The EnvironmentInfoFilter class that we implemented to grab information from the environment for logs is also built this way. We were able to get information about the Kubernetes deployment from the logs without having to modify the code.\n",
    "\n",
    "The LoggingDecorator class is designed to work with MLModel classes, and this is the only hard requirement of the code. This makes the decorator very portable, because we are able to deploy it inside of any other model deployment service we may choose to build in the future. For example, we can just as easily decorate an MLModel instance running inside of an gRPC service, since the decorator would work exactly the same way. This is due to interface-driven approach that we took when designing the MLModel interface.\n",
    "\n",
    "We added logging to the ML model instance from the \"outside\" and we were not able to access information about the internals of the model. This is a limitation of the decorator approach to logging which only has access to the model inputs, model outputs, and exceptions raised by the model. This approach is best used to add logging functionality to an ML model implementation that we do not control, or in simple situations in which the limitations of the approach do not affect us. If any logging of internal model state is needed, we'll need to generate logs from within the MLModel class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7740f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
